\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{compling}
\citation{Bolukbasi2016man,Caliskan2017semanticsBiases,Gonen2019lipstick,Lauscher2019multidimensional,Garg2018years,Manzini2019blackToCriminal}
\citation{Mikolov2013efficient}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}\protected@file@percent }
\newlabel{introduction}{{1}{1}{Introduction}{section.0.1}{}}
\citation{Pennington2014Glove}
\citation{Rabinovich2018Reddit}
\citation{Caliskan2017semanticsBiases}
\citation{Manzini2019blackToCriminal}
\citation{Caliskan2017semanticsBiases}
\citation{Manzini2019blackToCriminal}
\citation{Garg2018years}
\citation{Bolukbasi2016man}
\citation{Bolukbasi2016man}
\citation{Gonen2019lipstick}
\citation{Gonen2019lipstick}
\citation{Bolukbasi2016man}
\citation{Nissim2020fair}
\@writefile{toc}{\contentsline {section}{\numberline {2}Two measures of bias: WEAT and MAC}{3}{section.0.2}\protected@file@percent }
\newlabel{two-measures-of-bias-weat-and-mac}{{2}{3}{Two measures of bias: WEAT and MAC}{section.0.2}{}}
\newlabel{sec:two}{{2}{3}{Two measures of bias: WEAT and MAC}{section.0.2}{}}
\citation{Caliskan2017semanticsBiases}
\citation{Nosek2002harvesting}
\citation{Caliskan2017semanticsBiases}
\citation{Lauscher2019multidimensional}
\citation{Garg2018years}
\newlabel{eq:stAB}{{1}{4}{Two measures of bias: WEAT and MAC}{equation.0.2.1}{}}
\newlabel{eq:sXYAB}{{2}{4}{Two measures of bias: WEAT and MAC}{equation.0.2.2}{}}
\newlabel{eq:weat}{{3}{4}{Two measures of bias: WEAT and MAC}{equation.0.2.3}{}}
\citation{Manzini2019blackToCriminal}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A simple example of a two-class set-up. Two groups of protected words, $X$ and $Y$ with two stereotypical attribute sets. An example of \textsf  {WEAT} calculations follows.\relax }}{5}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{eq:WEATexample}{{1}{5}{A simple example of a two-class set-up. Two groups of protected words, $X$ and $Y$ with two stereotypical attribute sets. An example of \textsf {WEAT} calculations follows.\relax }{figure.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Sample 15 rows of the religion dataset. The whole dataset has 15 unique protected words ($T$), and 11 unique attributes divided between 3 attribute sets ($A_1=\mathsf  {jewStereotype}, A_2= \mathsf  {christianStereotype}, A_3=\mathsf  {muslimStereotype}$). $\mathcal  {A}$ consists of these three sets, $\mathcal  {A}= \{A_1, A_2, A_3\}$. The whole dataset has $15\times 11 = 165$ rows.\relax }}{6}{table.caption.6}\protected@file@percent }
\newlabel{tab:religionOriginal}{{1}{6}{Sample 15 rows of the religion dataset. The whole dataset has 15 unique protected words ($T$), and 11 unique attributes divided between 3 attribute sets ($A_1=\mathsf {jewStereotype}, A_2= \mathsf {christianStereotype}, A_3=\mathsf {muslimStereotype}$). $\mathcal {A}$ consists of these three sets, $\mathcal {A}= \{A_1, A_2, A_3\}$. The whole dataset has $15\times 11 = 165$ rows.\relax }{table.caption.6}{}}
\citation{Manzini2019blackToCriminal}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A small subset of the religion dataset. To each protected word in $T$ there corresponds one class of stereotypical attributes typically associated with it (and other classes of stereotypical attributes associated with different protected words).\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:MACexample}{{2}{7}{A small subset of the religion dataset. To each protected word in $T$ there corresponds one class of stereotypical attributes typically associated with it (and other classes of stereotypical attributes associated with different protected words).\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The associated mean average cosine similarity (\textsf  {MAC}) and p-values for debiasing methods for religious bias.\relax }}{7}{table.caption.8}\protected@file@percent }
\newlabel{tab:religionOriginal2}{{2}{7}{The associated mean average cosine similarity (\textsf {MAC}) and p-values for debiasing methods for religious bias.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Challenges to cosine-based bias metrics}{7}{section.0.3}\protected@file@percent }
\newlabel{challenges-to-cosine-based-bias-metrics}{{3}{7}{Challenges to cosine-based bias metrics}{section.0.3}{}}
\newlabel{sec:challenges}{{3}{7}{Challenges to cosine-based bias metrics}{section.0.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Interpretability issues}{7}{subsection.0.3.1}\protected@file@percent }
\newlabel{interpretability-issues}{{3.1}{7}{Interpretability issues}{subsection.0.3.1}{}}
\newlabel{subsec:interpretability}{{3.1}{7}{Interpretability issues}{subsection.0.3.1}{}}
\citation{Manzini2019blackToCriminal}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Empirical distributions of cosine similarities for the Religion word list used in the original paper. \relax }}{9}{figure.caption.9}\protected@file@percent }
\newlabel{fig:empirical0}{{3}{9}{Empirical distributions of cosine similarities for the Religion word list used in the original paper. \relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Empirical distributions of cosine similarities for the Gender word list used in the original paper. \relax }}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig:empirical2}{{4}{9}{Empirical distributions of cosine similarities for the Gender word list used in the original paper. \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Problems with pre-averaging}{9}{subsection.0.3.2}\protected@file@percent }
\newlabel{problems-with-pre-averaging}{{3.2}{9}{Problems with pre-averaging}{subsection.0.3.2}{}}
\newlabel{subsec:problems}{{3.2}{9}{Problems with pre-averaging}{subsection.0.3.2}{}}
\citation{Caliskan2017semanticsBiases}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Empirical distributions of cosine similarities for the Race word list used in the original paper. \relax }}{10}{figure.caption.11}\protected@file@percent }
\newlabel{fig:empirical3}{{5}{10}{Empirical distributions of cosine similarities for the Race word list used in the original paper. \relax }{figure.caption.11}{}}
\citation{Caliskan2017semanticsBiases}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Actual distances for the protected word \textsf  {muslim}. \relax }}{11}{figure.caption.12}\protected@file@percent }
\newlabel{fig:muslim}{{6}{11}{Actual distances for the protected word \textsf {muslim}. \relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Actual distances for the protected word \textsf  {priest}.\relax }}{12}{figure.caption.13}\protected@file@percent }
\newlabel{fig:priest}{{7}{12}{Actual distances for the protected word \textsf {priest}.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Bootstrapped distributions of test statistics and effect sizes in a random sample given the null hypothesis. We used a sample from the null model with N(0,.08) and 16 protected words, and then bootstrapped from it, following the original methodology. One particular bootstrapped sample is highlighted, and discussed further in the text. \relax }}{13}{figure.caption.14}\protected@file@percent }
\newlabel{fig:caliskanCalc}{{8}{13}{Bootstrapped distributions of test statistics and effect sizes in a random sample given the null hypothesis. We used a sample from the null model with N(0,.08) and 16 protected words, and then bootstrapped from it, following the original methodology. One particular bootstrapped sample is highlighted, and discussed further in the text. \relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Cosine distances to two attribute sets by protected word groups. Observe nothing unusual except for a few outliers.\relax }}{13}{figure.caption.15}\protected@file@percent }
\newlabel{fig:caliskanDistances}{{9}{13}{Cosine distances to two attribute sets by protected word groups. Observe nothing unusual except for a few outliers.\relax }{figure.caption.15}{}}
\citation{Caliskan2017semanticsBiases}
\citation{Manzini2019blackToCriminal}
\citation{Morey2015confidenceFallacy}
\citation{Hoekstra2014Misinterpretation}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Distributions of test statistics and effect sizes based on 10k simulations on the assumption of a null model in which all distances come from normal distribution with $\mu =0, \sigma = .08, n=10k$. We also mark the sample we have been using as an example.\relax }}{15}{figure.caption.16}\protected@file@percent }
\newlabel{fig:ourDistances}{{10}{15}{Distributions of test statistics and effect sizes based on 10k simulations on the assumption of a null model in which all distances come from normal distribution with $\mu =0, \sigma = .08, n=10k$. We also mark the sample we have been using as an example.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}A Bayesian approach to cosine-based bias}{15}{section.0.4}\protected@file@percent }
\newlabel{a-bayesian-approach-to-cosine-based-bias}{{4}{15}{A Bayesian approach to cosine-based bias}{section.0.4}{}}
\newlabel{sec:bayesian}{{4}{15}{A Bayesian approach to cosine-based bias}{section.0.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model construction}{15}{subsection.0.4.1}\protected@file@percent }
\newlabel{model-construction}{{4.1}{15}{Model construction}{subsection.0.4.1}{}}
\newlabel{subsec:model}{{4.1}{15}{Model construction}{subsection.0.4.1}{}}
\citation{kruschke2015bayesian,statrethinkingbook2020}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces At a general level, we will be estimating the coefficients for distances as grouped by whether they are between protected words and attributes coming from their respective associated/different/human/neutral attribute groups. At a more fine-grained level, for each protected word we will be estimating the proximity of that word to attributes that are associated with its respective stereotype, come from a different stereotype, or come from the human/neutral attribute lists.\relax }}{17}{figure.caption.17}\protected@file@percent }
\newlabel{fig:visDag}{{11}{17}{At a general level, we will be estimating the coefficients for distances as grouped by whether they are between protected words and attributes coming from their respective associated/different/human/neutral attribute groups. At a more fine-grained level, for each protected word we will be estimating the proximity of that word to attributes that are associated with its respective stereotype, come from a different stereotype, or come from the human/neutral attribute lists.\relax }{figure.caption.17}{}}
\citation{james2013introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Posterior predictive check}{18}{subsection.0.4.2}\protected@file@percent }
\newlabel{posterior-predictive-check}{{4.2}{18}{Posterior predictive check}{subsection.0.4.2}{}}
\newlabel{subsec:posterior}{{4.2}{18}{Posterior predictive check}{subsection.0.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Example of a posterior predictive check. (Top) Actual cosine distances are plotted against mean predictions with 89\% highest posterior density intervals. Notice that 90\% of actual values fall within the 89\% HPDI and 55\% of actual values fall into 50\% HPDI, which indicates appropriate performance of the model. The left-right alignment of different colors corresponds to the fact that cosine differences between elements of different categories differ, to some extent systematically (this will be studied in the results section). (Bottom) Densities of predicted and observed distances.\relax }}{19}{figure.caption.18}\protected@file@percent }
\newlabel{fig:posteriorCheck1}{{12}{19}{Example of a posterior predictive check. (Top) Actual cosine distances are plotted against mean predictions with 89\% highest posterior density intervals. Notice that 90\% of actual values fall within the 89\% HPDI and 55\% of actual values fall into 50\% HPDI, which indicates appropriate performance of the model. The left-right alignment of different colors corresponds to the fact that cosine differences between elements of different categories differ, to some extent systematically (this will be studied in the results section). (Bottom) Densities of predicted and observed distances.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results and discussion}{19}{section.0.5}\protected@file@percent }
\newlabel{results-and-discussion}{{5}{19}{Results and discussion}{section.0.5}{}}
\newlabel{sec:results}{{5}{19}{Results and discussion}{section.0.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Observations}{19}{subsection.0.5.1}\protected@file@percent }
\newlabel{observations}{{5.1}{19}{Observations}{subsection.0.5.1}{}}
\newlabel{subsec:observations}{{5.1}{19}{Observations}{subsection.0.5.1}{}}
\citation{Manzini2019blackToCriminal}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Rethinking debiasing}{21}{subsection.0.5.2}\protected@file@percent }
\newlabel{rethinking-debiasing}{{5.2}{21}{Rethinking debiasing}{subsection.0.5.2}{}}
\newlabel{subsec:rethinking}{{5.2}{21}{Rethinking debiasing}{subsection.0.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Mean cosine distances with 89\% highest posterior density intervals for the gender dataset before debiasing.\relax }}{22}{figure.caption.19}\protected@file@percent }
\newlabel{fig:empiricalPriorToDebiasing}{{13}{22}{Mean cosine distances with 89\% highest posterior density intervals for the gender dataset before debiasing.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Mean with 89\% highest posterior density intervals for gender after debiasing.\relax }}{23}{figure.caption.20}\protected@file@percent }
\newlabel{fig:empiricalDebiased}{{14}{23}{Mean with 89\% highest posterior density intervals for gender after debiasing.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Empirical distributions of cosine similarities before and after debiasing for the Religion word list used in the original paper.\relax }}{24}{figure.caption.21}\protected@file@percent }
\newlabel{fig:empiricalDebiased1}{{15}{24}{Empirical distributions of cosine similarities before and after debiasing for the Religion word list used in the original paper.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Potential objections}{24}{subsection.0.5.3}\protected@file@percent }
\newlabel{potential-objections}{{5.3}{24}{Potential objections}{subsection.0.5.3}{}}
\citation{Ethayarajh2020Bernstein}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Empirical distributions of cosine similarities before and after debiasing for the Gender word list used in the original paper.\relax }}{25}{figure.caption.22}\protected@file@percent }
\newlabel{fig:empiricalDebiased2}{{16}{25}{Empirical distributions of cosine similarities before and after debiasing for the Gender word list used in the original paper.\relax }{figure.caption.22}{}}
\citation{may-etal-2019-measuring}
\citation{husse-spitz-2022-mind}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Empirical distributions of cosine similarities before and after debiasing for the Race word list used in the original paper.\relax }}{26}{figure.caption.23}\protected@file@percent }
\newlabel{fig:empiricalDebiased3}{{17}{26}{Empirical distributions of cosine similarities before and after debiasing for the Race word list used in the original paper.\relax }{figure.caption.23}{}}
\citation{DBLP:journals/corr/abs-1811-07253}
\citation{schroder2021evaluating}
\citation{may-etal-2019-measuring}
\citation{Guo2021CEAT}
\citation{Lum2022Debiasing}
\citation{Ethayarajh2019understanding}
\citation{Bolukbasi2016man}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related works and conclusions}{27}{section.0.6}\protected@file@percent }
\newlabel{related-works-and-conclusions}{{6}{27}{Related works and conclusions}{section.0.6}{}}
\newlabel{sec:related}{{6}{27}{Related works and conclusions}{section.0.6}{}}
\citation{Ethayarajh2020Bernstein}
\citation{zhang2020robustness}
\citation{Du2021Assessing}
\citation{Goldfarb2021BiasNotCorrelate}
\citation{Spliethover2021BiasSilhouette}
\citation{JohnsonValueFree}
\citation{Garg2017hundredYears}
\citation{gordon2012reporting}
\@writefile{toc}{\contentsline {section}{\numberline {1}Appendix}{30}{appendix.A}\protected@file@percent }
\newlabel{appendix}{{1}{30}{Appendix}{appendix.A}{}}
\newlabel{sec:appendix}{{1}{30}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}A philosophical commentary}{30}{subsection.A.1}\protected@file@percent }
\newlabel{appendix:philosophical}{{1.1}{30}{A philosophical commentary}{subsection.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Visualizations}{32}{subsection.A.2}\protected@file@percent }
\newlabel{visualizations}{{1.2}{32}{Visualizations}{subsection.A.2}{}}
\newlabel{appendix:visualizations}{{1.2}{32}{Visualizations}{subsection.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Posterior predictive checks}{50}{subsection.A.3}\protected@file@percent }
\newlabel{posterior-predictive-checks}{{1.3}{50}{Posterior predictive checks}{subsection.A.3}{}}
\newlabel{appendix:posterior}{{1.3}{50}{Posterior predictive checks}{subsection.A.3}{}}
\citation{Manzini2019blackToCriminal}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Word lists}{52}{subsection.A.4}\protected@file@percent }
\newlabel{word-lists}{{1.4}{52}{Word lists}{subsection.A.4}{}}
\newlabel{appendix:word}{{1.4}{52}{Word lists}{subsection.A.4}{}}
\newlabel{lists-used-in-previous-research}{{1.4.1}{52}{Lists used in previous research}{subsubsection.A.4.1}{}}
\newlabel{appendix:manzini_word_lists}{{1.4.1}{52}{Lists used in previous research}{subsubsection.A.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Lists used in previous research}{52}{subsubsection.A.4.1}\protected@file@percent }
\newlabel{custom-lists-used-in-this-paper}{{1.4.2}{52}{\texorpdfstring {Custom lists used in this paper \label {app:custom}}{Custom lists used in this paper }}{subsubsection.A.4.2}{}}
\newlabel{app:custom}{{1.4.2}{52}{\texorpdfstring {Custom lists used in this paper \label {app:custom}}{Custom lists used in this paper }}{subsubsection.A.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Custom lists used in this paper }{52}{subsubsection.A.4.2}\protected@file@percent }
\bibdata{cosineReferences}
\bibcite{Bolukbasi2016man}{{1}{2016}{{Bolukbasi et~al.}}{{Bolukbasi, Chang, Zou, Saligrama, and Kalai}}}
\bibcite{Caliskan2017semanticsBiases}{{2}{2016}{{Caliskan, Bryson, and Narayanan}}{{}}}
\bibcite{Du2021Assessing}{{3}{2021}{{Du, Fang, and Nguyen}}{{}}}
\bibcite{Ethayarajh2020Bernstein}{{4}{2020}{{Ethayarajh}}{{}}}
\bibcite{Ethayarajh2019understanding}{{5}{2019}{{Ethayarajh, Duvenaud, and Hirst}}{{}}}
\bibcite{Garg2017hundredYears}{{6}{2017}{{Garg et~al.}}{{Garg, Schiebinger, Jurafsky, and Zou}}}
\bibcite{Garg2018years}{{7}{2018}{{Garg et~al.}}{{Garg, Schiebinger, Jurafsky, and Zou}}}
\bibcite{Goldfarb2021BiasNotCorrelate}{{8}{2021}{{Goldfarb-Tarrant et~al.}}{{Goldfarb-Tarrant, Marchant, Mu{\~n}oz~S{\'a}nchez, Pandya, and Lopez}}}
\bibcite{Gonen2019lipstick}{{9}{2019}{{Gonen and Goldberg}}{{}}}
\bibcite{gordon2012reporting}{{10}{2013}{{Gordon and Durme}}{{}}}
\bibcite{Guo2021CEAT}{{11}{2021}{{Guo and Caliskan}}{{}}}
\bibcite{Hoekstra2014Misinterpretation}{{12}{2014}{{Hoekstra et~al.}}{{Hoekstra, Morey, Rouder, and Wagenmakers}}}
\bibcite{husse-spitz-2022-mind}{{13}{2022}{{Husse and Spitz}}{{}}}
\bibcite{james2013introduction}{{14}{2013}{{James et~al.}}{{James, Witten, Hastie, Tibshirani et~al.}}}
\bibcite{JohnsonValueFree}{{15}{forthcoming}{{Johnson}}{{}}}
\bibcite{kruschke2015bayesian}{{16}{2015}{{Kruschke}}{{}}}
\bibcite{Lauscher2019multidimensional}{{17}{2019}{{Lauscher and Glavas}}{{}}}
\bibcite{Lum2022Debiasing}{{18}{2022}{{Lum, Zhang, and Bower}}{{}}}
\bibcite{Manzini2019blackToCriminal}{{19}{2019}{{Manzini et~al.}}{{Manzini, Lim, Tsvetkov, and Black}}}
\bibcite{may-etal-2019-measuring}{{20}{2019}{{May et~al.}}{{May, Wang, Bordia, Bowman, and Rudinger}}}
\bibcite{statrethinkingbook2020}{{21}{2020}{{McElreath}}{{}}}
\bibcite{Mikolov2013efficient}{{22}{2013}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{Morey2015confidenceFallacy}{{23}{2015}{{Morey et~al.}}{{Morey, Hoekstra, Rouder, Lee, and Wagenmakers}}}
\bibcite{Nissim2020fair}{{24}{2020}{{Nissim, van Noord, and van~der Goot}}{{}}}
\bibcite{Nosek2002harvesting}{{25}{2002}{{Nosek, Banaji, and Greenwald}}{{}}}
\bibcite{Pennington2014Glove}{{26}{2014}{{Pennington, Socher, and Manning}}{{}}}
\bibcite{Rabinovich2018Reddit}{{27}{2018}{{Rabinovich, Tsvetkov, and Wintner}}{{}}}
\bibcite{schroder2021evaluating}{{28}{2021}{{Schröder et~al.}}{{Schröder, Schulz, Kenneweg, Feldhans, Hinder, and Hammer}}}
\bibcite{Spliethover2021BiasSilhouette}{{29}{2021}{{Spliethöver and Wachsmuth}}{{}}}
\bibcite{DBLP:journals/corr/abs-1811-07253}{{30}{2018}{{Xiao and Wang}}{{}}}
\bibcite{zhang2020robustness}{{31}{2020}{{Zhang, Sneyd, and Stevenson}}{{}}}
\gdef \@abspage@last{55}
