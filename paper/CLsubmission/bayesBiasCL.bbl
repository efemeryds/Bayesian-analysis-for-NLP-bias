\begin{thebibliography}{31}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Bolukbasi et~al.(2016)Bolukbasi, Chang, Zou, Saligrama, and
  Kalai}]{Bolukbasi2016man}
Bolukbasi, Tolga, Kai{-}Wei Chang, James~Y. Zou, Venkatesh Saligrama, and Adam
  Kalai. 2016.
\newblock Man is to computer programmer as woman is to homemaker? debiasing
  word embeddings.
\newblock \emph{CoRR}, abs/1607.06520.

\bibitem[{Caliskan, Bryson, and Narayanan(2016)}]{Caliskan2017semanticsBiases}
Caliskan, Aylin, Joanna~J. Bryson, and Arvind Narayanan. 2016.
\newblock Semantics derived automatically from language corpora necessarily
  contain human biases.
\newblock \emph{CoRR}, abs/1608.07187.

\bibitem[{Du, Fang, and Nguyen(2021)}]{Du2021Assessing}
Du, Yupei, Qixiang Fang, and Dong Nguyen. 2021.
\newblock Assessing the reliability of word embedding gender bias measures.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 10012--10034, Association for
  Computational Linguistics, Online and Punta Cana, Dominican Republic.

\bibitem[{Ethayarajh(2020)}]{Ethayarajh2020Bernstein}
Ethayarajh, Kawin. 2020.
\newblock Is your classifier actually biased? measuring fairness under
  uncertainty with bernstein bounds.
\newblock \emph{CoRR}, abs/2004.12332.

\bibitem[{Ethayarajh, Duvenaud, and Hirst(2019)}]{Ethayarajh2019understanding}
Ethayarajh, Kawin, David Duvenaud, and Graeme Hirst. 2019.
\newblock Understanding undesirable word embedding associations.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 1696--1705.

\bibitem[{Garg et~al.(2017)Garg, Schiebinger, Jurafsky, and
  Zou}]{Garg2017hundredYears}
Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2017.
\newblock Word embeddings quantify 100 years of gender and ethnic stereotypes.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115.

\bibitem[{Garg et~al.(2018)Garg, Schiebinger, Jurafsky, and
  Zou}]{Garg2018years}
Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018.
\newblock Word embeddings quantify 100 years of gender and ethnic stereotypes.
\newblock \emph{Proceedings of the National Academy of Sciences},
  115(16):E3635--E3644.

\bibitem[{Goldfarb-Tarrant et~al.(2021)Goldfarb-Tarrant, Marchant,
  Mu{\~n}oz~S{\'a}nchez, Pandya, and Lopez}]{Goldfarb2021BiasNotCorrelate}
Goldfarb-Tarrant, Seraphina, Rebecca Marchant, Ricardo Mu{\~n}oz~S{\'a}nchez,
  Mugdha Pandya, and Adam Lopez. 2021.
\newblock Intrinsic bias metrics do not correlate with application bias.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 1926--1940,
  Association for Computational Linguistics, Online.

\bibitem[{Gonen and Goldberg(2019)}]{Gonen2019lipstick}
Gonen, Hila and Yoav Goldberg. 2019.
\newblock Lipstick on a pig: {D}ebiasing methods cover up systematic gender
  biases in word embeddings but do not remove them.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 609--614, Association
  for Computational Linguistics, Minneapolis, Minnesota.

\bibitem[{Gordon and Durme(2013)}]{gordon2012reporting}
Gordon, Jonathan and Benjamin Durme. 2013.
\newblock Reporting bias and knowledge acquisition.
\newblock pages 25--30.

\bibitem[{Guo and Caliskan(2021)}]{Guo2021CEAT}
Guo, Wei and Aylin Caliskan. 2021.
\newblock Detecting emergent intersectional biases: Contextualized word
  embeddings contain a distribution of human-like biases.
\newblock In \emph{Proceedings of the 2021 {AAAI}/{ACM} Conference on {AI},
  Ethics, and Society}, {ACM}.

\bibitem[{Hoekstra et~al.(2014)Hoekstra, Morey, Rouder, and
  Wagenmakers}]{Hoekstra2014Misinterpretation}
Hoekstra, Rink, Richard~D. Morey, Jeffrey~N. Rouder, and Eric-Jan Wagenmakers.
  2014.
\newblock Robust misinterpretation of confidence intervals.
\newblock \emph{Psychonomic Bulletin \& Review}, 21(5):1157--1164.

\bibitem[{Husse and Spitz(2022)}]{husse-spitz-2022-mind}
Husse, Silke and Andreas Spitz. 2022.
\newblock Mind your bias: A critical review of bias detection methods for
  contextual language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 4212--4234, Association for Computational Linguistics, Abu
  Dhabi, United Arab Emirates.

\bibitem[{James et~al.(2013)James, Witten, Hastie, Tibshirani
  et~al.}]{james2013introduction}
James, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et~al. 2013.
\newblock \emph{An introduction to statistical learning}, volume 112.
\newblock Springer.

\bibitem[{Johnson(forthcoming)}]{JohnsonValueFree}
Johnson, Gabbrielle. forthcoming.
\newblock Are algorithms value-free? feminist theoretical virtues in machine
  learning.
\newblock \emph{Journal Moral Philosophy}.

\bibitem[{Kruschke(2015)}]{kruschke2015bayesian}
Kruschke, John. 2015.
\newblock \emph{Doing {B}ayesian Data Analysis (Second Edition)}.
\newblock Academic Press, Boston.

\bibitem[{Lauscher and Glavas(2019)}]{Lauscher2019multidimensional}
Lauscher, Anne and Goran Glavas. 2019.
\newblock Are we consistently biased? multidimensional analysis of biases in
  distributional word vectors.
\newblock \emph{CoRR}, abs/1904.11783.

\bibitem[{Lum, Zhang, and Bower(2022)}]{Lum2022Debiasing}
Lum, Kristian, Yunfeng Zhang, and Amanda Bower. 2022.
\newblock De-biasing “bias” measurement.
\newblock In \emph{Proceedings of the 2022 ACM Conference on Fairness,
  Accountability, and Transparency}, FAccT '22, page 379–389, Association for
  Computing Machinery, New York, NY, USA.

\bibitem[{Manzini et~al.(2019)Manzini, Lim, Tsvetkov, and
  Black}]{Manzini2019blackToCriminal}
Manzini, Thomas, Yao~Chong Lim, Yulia Tsvetkov, and Alan~W Black. 2019.
\newblock Black is to criminal as caucasian is to police: Detecting and
  removing multiclass bias in word embeddings.

\bibitem[{May et~al.(2019)May, Wang, Bordia, Bowman, and
  Rudinger}]{may-etal-2019-measuring}
May, Chandler, Alex Wang, Shikha Bordia, Samuel~R. Bowman, and Rachel Rudinger.
  2019.
\newblock On measuring social biases in sentence encoders.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 622--628, Association
  for Computational Linguistics, Minneapolis, Minnesota.

\bibitem[{McElreath(2020)}]{statrethinkingbook2020}
McElreath, Richard. 2020.
\newblock \emph{Statistical Rethinking: A Bayesian Course with Examples in R
  and Stan, 2nd Edition}, 2 edition.
\newblock CRC Press.

\bibitem[{Mikolov et~al.(2013)Mikolov, Chen, Corrado, and
  Dean}]{Mikolov2013efficient}
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
\newblock Efficient estimation of word representations in vector space.

\bibitem[{Morey et~al.(2015)Morey, Hoekstra, Rouder, Lee, and
  Wagenmakers}]{Morey2015confidenceFallacy}
Morey, Richard, Rink Hoekstra, Jeffrey Rouder, Michael Lee, and EJ~Wagenmakers.
  2015.
\newblock The fallacy of placing confidence in confidence intervals.
\newblock \emph{Psychonomic Bulletin \& Review}.

\bibitem[{Nissim, van Noord, and van~der Goot(2020)}]{Nissim2020fair}
Nissim, Malvina, Rik van Noord, and Rob van~der Goot. 2020.
\newblock Fair is better than sensational: Man is to doctor as woman is to
  doctor.
\newblock \emph{Computational Linguistics}, 46(2):487--497.

\bibitem[{Nosek, Banaji, and Greenwald(2002)}]{Nosek2002harvesting}
Nosek, Brian~A., Mahzarin~R. Banaji, and Anthony~G. Greenwald. 2002.
\newblock Harvesting implicit group attitudes and beliefs from a demonstration
  web site.
\newblock \emph{Group Dynamics: Theory, Research, and Practice}, 6(1):101--115.

\bibitem[{Pennington, Socher, and Manning(2014)}]{Pennington2014Glove}
Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014.
\newblock {G}lo{V}e: Global vectors for word representation.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing ({EMNLP})}, pages 1532--1543, Association for
  Computational Linguistics, Doha, Qatar.

\bibitem[{Rabinovich, Tsvetkov, and Wintner(2018)}]{Rabinovich2018Reddit}
Rabinovich, Ella, Yulia Tsvetkov, and Shuly Wintner. 2018.
\newblock Native language cognate effects on second language lexical choice.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  6:329--342.

\bibitem[{Schröder et~al.(2021)Schröder, Schulz, Kenneweg, Feldhans, Hinder,
  and Hammer}]{schroder2021evaluating}
Schröder, Sarah, Alexander Schulz, Philip Kenneweg, Robert Feldhans, Fabian
  Hinder, and Barbara Hammer. 2021.
\newblock Evaluating metrics for bias in word embeddings.

\bibitem[{Spliethöver and Wachsmuth(2021)}]{Spliethover2021BiasSilhouette}
Spliethöver, Maximilian and Henning Wachsmuth. 2021.
\newblock Bias silhouette analysis: Towards assessing the quality of bias
  metrics for word embedding models.
\newblock In \emph{Proceedings of the Thirtieth International Joint Conference
  on Artificial Intelligence, {IJCAI-21}}, pages 552--559, International Joint
  Conferences on Artificial Intelligence Organization.
\newblock Main Track.

\bibitem[{Xiao and Wang(2018)}]{DBLP:journals/corr/abs-1811-07253}
Xiao, Yijun and William~Yang Wang. 2018.
\newblock Quantifying uncertainties in natural language processing tasks.
\newblock \emph{CoRR}, abs/1811.07253.

\bibitem[{Zhang, Sneyd, and Stevenson(2020)}]{zhang2020robustness}
Zhang, Haiyang, Alison Sneyd, and Mark Stevenson. 2020.
\newblock Robustness and reliability of gender bias assessment in word
  embeddings: The role of base pairs.

\end{thebibliography}
